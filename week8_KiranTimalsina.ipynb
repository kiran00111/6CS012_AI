{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OtKHMm_xaS4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e50ce4-37b5-43b1-b202-cdff24742851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "FUd7Vji2gNnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Trumptweets/trum_tweet_sentiment_analysis.csv')"
      ],
      "metadata": {
        "id": "0RKJfkF2gkCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "auZkDDZ7gmxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Function for Text Cleaning:**\n",
        "\n",
        "Implement a Helper Function as per Text Preprocessing Notebook and Complete the following pipeline"
      ],
      "metadata": {
        "id": "MZOBjkOdgtQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build a Text Cleaning Pipeline**"
      ],
      "metadata": {
        "id": "LonBCcc5hDLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "XKmhAHCqhKXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning_pipeline(dataset, rule = \"lemmatize\"):\n",
        "  \"\"\"\n",
        "  This...\n",
        "  \"\"\"\n",
        "  # Convert the input to small/lower order.\n",
        "  data = dataset.lower()\n",
        "  # Remove URLs\n",
        "  data =re.sub(r\"http\\S+|www\\S+|https\\S+\", '',data, flags=re.MULTILINE)\n",
        "  # Remove emojis\n",
        "  data = re.sub(r\"[\"\n",
        "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                        u\"\\u2702-\\u27B0\"          # dingbats\n",
        "                        u\"\\u24C2-\\U0001F251\"      # enclosed characters\n",
        "                        \"]+\",\n",
        "                        r' ', data, flags=re.UNICODE)\n",
        "  # Remove all other unwanted characters.\n",
        "  data = re.sub(\"[^0-9A-Za-z ]\", \"\" , data)\n",
        "  #Remove all mentions:\n",
        "  data = re.sub(\"@[A-Za-z0-9_]+\",\" \", data)\n",
        "  # Create tokens.\n",
        "  tokens = data.split()\n",
        "  # Remove stopwords:\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  result_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      result_tokens.append(token)\n",
        "  if rule == \"lemmatize\":\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in result_tokens]\n",
        "  elif rule == \"stem\":\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in result_tokens]\n",
        "  else:\n",
        "    print(\"Pick between lemmatize or stem\")\n",
        "\n",
        "\n",
        "  return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "hbR0zzt4hNsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Classification using Machine Learning Models**"
      ],
      "metadata": {
        "id": "mCpWHdgQhUzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìù Instructions: Trump Tweet Sentiment Classification**"
      ],
      "metadata": {
        "id": "WqpSTfMLhWmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the Dataset\n",
        "Load the dataset named \"trump_tweet_sentiment_analysis.csv\" using pandas. Ensure the dataset contains at least two columns: \"text\" and \"label\".\n",
        "\n",
        "2.Text Cleaning and Tokenization\n",
        "Apply a text preprocessing pipeline to the \"text\" column. This should include:\n",
        "\n",
        "Lowercasing the text\n",
        "Removing URLs, mentions, punctuation, and special characters\n",
        "Removing stopwords\n",
        "Tokenization (optional: stemming or lemmatization)\n",
        "\"Complete the above function\"\n",
        "\n",
        "3.Train-Test Split\n",
        "Split the cleaned and tokenized dataset into training and testing sets using train_test_split from sklearn.model_selection.\n",
        "\n",
        "4.TF-IDF Vectorization\n",
        "Import and use the TfidfVectorizer from sklearn.feature_extraction.text to transform the training and testing texts into numerical feature vectors.\n",
        "\n",
        "5. Model Training and Evaluation\n",
        "Import Logistic Regression (or any machine learning model of your choice) from sklearn.linear_model. Train it on the TF-IDF-embedded training data, then evaluate it using the test set.\n",
        "\n",
        "Print the classification report using classification_report from sklearn.metrics."
      ],
      "metadata": {
        "id": "sMaawZruhggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['text'].apply(lambda x: text_cleaning_pipeline(x, rule = \"lemmatize\"))"
      ],
      "metadata": {
        "id": "QkBXPJ8wRZOs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "rnWG53mfTHPj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'], df['Sentiment'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "zGoaL6kFTQb1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "wvFiG5pbTTrm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "y_pred = model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "vosjvL2CTXXR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "urL5PyFkTa9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ee9dd8-e6fa-4d34-ca88-6596fa8c2930"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96    248563\n",
            "           1       0.94      0.91      0.92    121462\n",
            "\n",
            "    accuracy                           0.95    370025\n",
            "   macro avg       0.95      0.94      0.94    370025\n",
            "weighted avg       0.95      0.95      0.95    370025\n",
            "\n"
          ]
        }
      ]
    }
  ]
}